{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOa1MZ5qykh/vxs6YMLx04S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TryDovick/MosiNLP/blob/main/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWQH0EgeXg_w",
        "outputId": "4478ccf3-4c6c-4ed0-dcfb-86ba6d26840f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ],
      "source": [
        "pip install pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rNkGgOcX3df",
        "outputId": "586f477c-a99d-47df-8f68-60dfade89a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nmaS4K3JYFg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример выполнения Лемматизации\n",
        "1. Инициалзируем лемматизатор через через библиотеку pymorphy3 и класс MorphAnalyzer  в модуль morph\n",
        "2. Проводим токенизацию текста word_tokenize\n",
        "3. print(\" \".join(lemmatize(text))) - выводим данные"
      ],
      "metadata": {
        "id": "4iL20ATZYd6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "  morph = pymorphy3.MorphAnalyzer()\n",
        "  tokens = word_tokenize(text)\n",
        "  return [morph.parse(word)[0].normal_form for word in tokens]\n",
        "\n",
        "text = \"Даже в самой прозрачной воде можно утонуть если она глубока\"\n",
        "\n",
        "print(\" \".join(lemmatize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVe7hENsYRl5",
        "outputId": "bfdc8ef6-0c5f-47bd-ca02-d3f9e211013a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "даже в сам прозрачный вода можно утонуть если она глубокий\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример выполнения Стемминга\n",
        "SnowballStemmer(\"russian\") – создает стеммер для русского языка.\n",
        "\n",
        "word_tokenize(text) – разбивает текст на отдельные слова (токены).\n",
        "\n",
        "stemmer.stem(word) – применяет стемминг к каждому слову.\n",
        "\n",
        "\" \".join(...) – объединяет стеммированные слова обратно в строку.\n",
        "\n"
      ],
      "metadata": {
        "id": "-facZ15ZZtT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text):\n",
        "  stemmer = SnowballStemmer(\"russian\")\n",
        "  tokens = word_tokenize(text)\n",
        "  return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "text = \"Даже в самой прозрачной воде можно утонуть если она глубока\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\" \".join(stemming(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CqeXDxEbgrJ",
        "outputId": "19c8b4b9-a5bc-470a-fece-b8cb47045f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "даж в сам прозрачн вод можн утонут есл он глубок\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенезируем списки и словари ASCII-символов (от 0 до 127)"
      ],
      "metadata": {
        "id": "fSPPrUlec1q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_ascii():\n",
        "    \"\"\"Возвращает список ASCII-символов (от 0 до 127).\"\"\"\n",
        "    return [chr(i) for i in range(128)]\n",
        "\n",
        "def vector_ascii():\n",
        "    \"\"\"Возвращает словарь, где ключи — коды ASCII, а значения — символы.\"\"\"\n",
        "    return {i: chr(i) for i in range(128)}\n",
        "\n",
        "\n",
        "print(\"Список ASCII-символов:\", token_ascii())\n",
        "print(\"\\nСловарь ASCII-символов:\", vector_ascii())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JabArwLyccS0",
        "outputId": "c7ae9aca-5ed5-42dc-f233-4d3db59ea114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Список ASCII-символов: ['\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07', '\\x08', '\\t', '\\n', '\\x0b', '\\x0c', '\\r', '\\x0e', '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a', '\\x1b', '\\x1c', '\\x1d', '\\x1e', '\\x1f', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f']\n",
            "\n",
            "Словарь ASCII-символов: {0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 127: '\\x7f'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проделаем операцию с текстом"
      ],
      "metadata": {
        "id": "8Ll20bxjddVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"russian\")\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "def stemming(text):\n",
        "    \"\"\"Стемминг текста (приведение слов к корневой форме).\"\"\"\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "def lemmatize(text):\n",
        "    \"\"\"Лемматизация текста (приведение слов к нормальной форме).\"\"\"\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [morph.parse(word)[0].normal_form for word in tokens]\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Разбивает текст на отдельные символы.\"\"\"\n",
        "    return [token for token in text]\n",
        "text = \"Векторизация текста — важный этап в обработке естественного языка. Она преобразует слова в числовые представления, что позволяет машинам понимать и анализировать текст.\"\n",
        "\n",
        "print(\"Исходный текст:\", text)\n",
        "\n",
        "\n",
        "lemmatized = lemmatize(text)\n",
        "stemmed = stemming(text)\n",
        "\n",
        "print(\"\\nЛемматизированный текст:\", \" \".join(lemmatized))\n",
        "print(\"Стеммированный текст:\", \" \".join(stemmed))\n",
        "\n",
        "\n",
        "print(\"\\nТокенизация после лемматизации:\", tokenize(\" \".join(lemmatized)))\n",
        "print(\"Токенизация после стемминга:\", tokenize(\" \".join(stemmed)))\n",
        "\n",
        "\n",
        "print(\"\\nВекторизация после лемматизации:\", vectorize(\" \".join(lemmatized)))\n",
        "print(\"Векторизация после стемминга:\", vectorize(\" \".join(stemmed)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM3ZvV3idbbS",
        "outputId": "e31926c1-1eb0-47fa-f6b5-ba9075bfa8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст: Векторизация текста — важный этап в обработке естественного языка. Она преобразует слова в числовые представления, что позволяет машинам понимать и анализировать текст.\n",
            "\n",
            "Лемматизированный текст: векторизация текст — важный этап в обработка естественный язык . она преобразовать слово в числовой представление , что позволять машина понимать и анализировать текст .\n",
            "Стеммированный текст: векторизац текст — важн этап в обработк естествен язык . он преобраз слов в числов представлен , что позволя машин понима и анализирова текст .\n",
            "\n",
            "Токенизация после лемматизации: ['в', 'е', 'к', 'т', 'о', 'р', 'и', 'з', 'а', 'ц', 'и', 'я', ' ', 'т', 'е', 'к', 'с', 'т', ' ', '—', ' ', 'в', 'а', 'ж', 'н', 'ы', 'й', ' ', 'э', 'т', 'а', 'п', ' ', 'в', ' ', 'о', 'б', 'р', 'а', 'б', 'о', 'т', 'к', 'а', ' ', 'е', 'с', 'т', 'е', 'с', 'т', 'в', 'е', 'н', 'н', 'ы', 'й', ' ', 'я', 'з', 'ы', 'к', ' ', '.', ' ', 'о', 'н', 'а', ' ', 'п', 'р', 'е', 'о', 'б', 'р', 'а', 'з', 'о', 'в', 'а', 'т', 'ь', ' ', 'с', 'л', 'о', 'в', 'о', ' ', 'в', ' ', 'ч', 'и', 'с', 'л', 'о', 'в', 'о', 'й', ' ', 'п', 'р', 'е', 'д', 'с', 'т', 'а', 'в', 'л', 'е', 'н', 'и', 'е', ' ', ',', ' ', 'ч', 'т', 'о', ' ', 'п', 'о', 'з', 'в', 'о', 'л', 'я', 'т', 'ь', ' ', 'м', 'а', 'ш', 'и', 'н', 'а', ' ', 'п', 'о', 'н', 'и', 'м', 'а', 'т', 'ь', ' ', 'и', ' ', 'а', 'н', 'а', 'л', 'и', 'з', 'и', 'р', 'о', 'в', 'а', 'т', 'ь', ' ', 'т', 'е', 'к', 'с', 'т', ' ', '.']\n",
            "Токенизация после стемминга: ['в', 'е', 'к', 'т', 'о', 'р', 'и', 'з', 'а', 'ц', ' ', 'т', 'е', 'к', 'с', 'т', ' ', '—', ' ', 'в', 'а', 'ж', 'н', ' ', 'э', 'т', 'а', 'п', ' ', 'в', ' ', 'о', 'б', 'р', 'а', 'б', 'о', 'т', 'к', ' ', 'е', 'с', 'т', 'е', 'с', 'т', 'в', 'е', 'н', ' ', 'я', 'з', 'ы', 'к', ' ', '.', ' ', 'о', 'н', ' ', 'п', 'р', 'е', 'о', 'б', 'р', 'а', 'з', ' ', 'с', 'л', 'о', 'в', ' ', 'в', ' ', 'ч', 'и', 'с', 'л', 'о', 'в', ' ', 'п', 'р', 'е', 'д', 'с', 'т', 'а', 'в', 'л', 'е', 'н', ' ', ',', ' ', 'ч', 'т', 'о', ' ', 'п', 'о', 'з', 'в', 'о', 'л', 'я', ' ', 'м', 'а', 'ш', 'и', 'н', ' ', 'п', 'о', 'н', 'и', 'м', 'а', ' ', 'и', ' ', 'а', 'н', 'а', 'л', 'и', 'з', 'и', 'р', 'о', 'в', 'а', ' ', 'т', 'е', 'к', 'с', 'т', ' ', '.']\n",
            "\n",
            "Векторизация после лемматизации: {1074: 'в', 1077: 'е', 1082: 'к', 1090: 'т', 1086: 'о', 1088: 'р', 1080: 'и', 1079: 'з', 1072: 'а', 1094: 'ц', 1103: 'я', 32: ' ', 1089: 'с', 8212: '—', 1078: 'ж', 1085: 'н', 1099: 'ы', 1081: 'й', 1101: 'э', 1087: 'п', 1073: 'б', 46: '.', 1100: 'ь', 1083: 'л', 1095: 'ч', 1076: 'д', 44: ',', 1084: 'м', 1096: 'ш'}\n",
            "Векторизация после стемминга: {1074: 'в', 1077: 'е', 1082: 'к', 1090: 'т', 1086: 'о', 1088: 'р', 1080: 'и', 1079: 'з', 1072: 'а', 1094: 'ц', 32: ' ', 1089: 'с', 8212: '—', 1078: 'ж', 1085: 'н', 1101: 'э', 1087: 'п', 1073: 'б', 1103: 'я', 1099: 'ы', 46: '.', 1083: 'л', 1095: 'ч', 1076: 'д', 44: ',', 1084: 'м', 1096: 'ш'}\n"
          ]
        }
      ]
    }
  ]
}